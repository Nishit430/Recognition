grid.arrange(hist1, hist2, hist3, hist4, ncol=2, nrow=2)
indf <- read.csv("C:\\Users\\nksay\\Documents\\EXL Analytics\\Data.csv")
library(ggplot2)
library(corrplot)
library(MASS)
library(dplyr)
library(ROCR)
library(randomForest)
str(indf)
indf <- data.frame(indf)
ggplot(data = indf, aes(x = Attrition_Flag)) + geom_histogram(aes(y = (..count..)/sum(..count..))) +xlim(c(-0.09,1.01))
indf$flag_attrition <- as.factor(indf$Attrition_Flag)
indf$flag_Vol_17_COA_Win <- ifelse(indf$Vol_17_COA_Win > 0, 1, 0)
indf$flag_Vol_17_COA <- ifelse(indf$Vol_17_COA > 0, 1, 0)
indf$log_Vol_17_COA_Win <- log(indf$Vol_17_COA_Win, base = exp(1))
indf$log_Vol_17_GeoPod <- log(indf$Vol_17_GeoPod, base = exp(1))
ggplot(data = indf, aes(x = log_Vol_17_GeoPod)) + geom_histogram()
ggplot(data = indf, aes(y = indf$log_Vol_17_COA, col = indf$flag_attrition)) + geom_boxplot()
ggplot(data = indf, aes(y = indf$log_Vol_17_COA_Win, col = indf$flag_attrition)) + geom_boxplot()
ggplot(data =indf, aes(x = Vol_17_COA_Win, col = flag_attrition)) + geom_histogram()+ xlim(c(1,300))
ggplot(data =indf, aes(x = Vol_17_COA, col = flag_attrition)) + geom_histogram(binwidth = 10) + xlim(c(1,500)) + ylim(c(0,150))
ggplot(data =indf, aes(x = factor(flag_Vol_17_COA_Win), col = flag_attrition)) + geom_bar(position = "fill")
ggplot(data =indf, aes(x = factor(flag_Vol_17_COA), col = flag_attrition)) + geom_bar(position = "fill")
str(indf_use)
ggplot(data =indf, aes(x = factor(flag_Vol_17_COA), col = flag_attrition)) + geom_bar(position = "fill")
ggplot(data =indf, aes(x = factor(flag_Vol_17_COA), fill = flag_attrition)) + geom_bar(position = "fill")
data_load <- read.csv("C:\\Users\\nksay\\Documents\\EXL Analytics\\Data.csv")
data_balanced_over1 <- ovun.sample(Attrition_Flag ~ ., data = data_load, method = "under",N = 12000, na.action = na.pass)$data
str(data_balanced_over1)
library(ROSE)
library(ggplot2)
library(corrplot)
library(MASS)
library(dplyr)
library(ROCR)
library(randomForest)
library(mice)
library(caret)
library(caTools)
library(outliers)
library(tidyverse)
library(xgboost)
library(caret)
library(Matrix)
require(data.table)
## Splitting the data into training and testing data
set.seed(31)
ptr <- sample.split(data_balanced_over1$Attrition_Flag, SplitRatio = 0.7)
head(ptr)
df <- subset(data_balanced_over1, ptr == TRUE)
test_data <- subset(data_balanced_over1, ptr == FALSE)
## Splitting the data into training and testing data
set.seed(31)
ptr <- sample.split(data_balanced_over1$Attrition_Flag, SplitRatio = 0.7)
data_balanced_over1 <- ovun.sample(Attrition_Flag ~ ., data = data_load, method = "under",N = 12000, na.action = na.pass)$data
str(data_balanced_over1)
## Splitting the data into training and testing data
set.seed(31)
ptr <- sample.split(data_balanced_over1$Attrition_Flag, SplitRatio = 0.7)
head(ptr)
df <- subset(data_balanced_over1, ptr == TRUE)
test_data <- subset(data_balanced_over1, ptr == FALSE)
#Let's start with looking at the variables and framing the data as data.fram
str(df)
df$ï..UNIQUE_ID <- NULL
df <- data.frame(df)
str(df)
# Making correction to data types
df$Attrition_Flag <- as.factor(df$Attrition_Flag)
df$Vol_16_D <- as.integer(df$Vol_16_D)
df$Vol_16_GeoPod_HR <- as.integer(df$Vol_16_GeoPod_HR)
df$Vol_16_F <- as.integer(df$Vol_16_F)
df$Vol_16_G <- as.integer(df$Vol_16_G)
df$Ind_2013_pushback <- as.factor(df$Ind_2013_pushback)
df$Ind_2014_pushback <- as.factor(df$Ind_2014_pushback)
df$Ind_2015_pushback <- as.factor(df$Ind_2015_pushback)
df$Ind_2016_pushback <- as.factor(df$Ind_2016_pushback)
df$Ind_2017_pushback <- as.factor(df$Ind_2017_pushback)
str(df)
##looking for NAs
sapply(df, function(x) sum(is.na(x)))
## same no. of NAs are found in Datacorp market share, Other competitors market share and Major competitiors
## There is a high chance of correlation between such variables
## Checking for correlation by dropping NAs
cor_mshare <- cor(df[,c(34:36)], use = "pairwise.complete.obs")
corrplot.mixed(cor_mshare)
str(df)
## High cor between data corp and others, thus we only keep datacorp variable
df$Major_competitor_market_share <- NULL
df$Other_competitor_market_share <- NULL
init = mice(df, maxit=0)
meth = init$method
predM = init$predictorMatrix
init$method
predM[, c("Attrition_Flag")]=0
predM[,c("Sales_Person_Jun_17")] = 0
predM[,c("Sales_Person_Dec_17")] = 0
meth[c("Count_Entities")]=""
meth[c("Count_Jurisdicion")]=""
meth[c("Tenure_months")]="cart"
meth[c("DataCorp_Market_Share")]="cart"
imputed = mice(df, method=meth, predictorMatrix=predM, m=5)
str(imputed)
imputed1 <- mice::complete(imputed)
nrow(imputed1)
sapply(imputed1, function(x) sum(is.na(x)))
df <- imputed1
df <- na.omit(df)
## CART based DT
fit1 <- rpart(Attrition_Flag ~ ., data = df)
data_load <- read.csv("C:\\Users\\nksay\\Documents\\EXL Analytics\\Data.csv")
library(ROSE)
library(ggplot2)
library(corrplot)
library(MASS)
library(dplyr)
library(ROCR)
library(randomForest)
library(mice)
library(caret)
library(caTools)
library(outliers)
library(tidyverse)
library(xgboost)
library(caret)
library(Matrix)
library(data.table)
library(glmnet)
library(rpart)
data_balanced_over1 <- ovun.sample(Attrition_Flag ~ ., data = data_load, method = "under",N = 12000, na.action = na.pass)$data
str(data_balanced_over1)
## Splitting the data into training and testing data
set.seed(31)
ptr <- sample.split(data_balanced_over1$Attrition_Flag, SplitRatio = 0.7)
head(ptr)
df <- subset(data_balanced_over1, ptr == TRUE)
test_data <- subset(data_balanced_over1, ptr == FALSE)
test_data$ï..UNIQUE_ID <- NULL
df$ï..UNIQUE_ID <- NULL
## CART based DT
fit1 <- rpart(Attrition_Flag ~ ., data = df)
plot(fit1);text(fit1);
test$t<-predict(fit1,type='class',test_data)
test$t<-predict(fit1,type='class',test_data)
?predict
test_data$t<-predict(fit1,type='class',test_data)
?predict.rpart
test_data$t<-predict(fit1,type='prob',test_data)
test_data$Attrition_Flag<-predict(fit1,type='prob',test_data)
predict(fit1,type='prob',test_data)
predict(fit1,type='class',test_data)
plot(fit1);text(fit1, pretty = 0);
## CART based DT
fit1 <- rpart(Attrition_Flag ~ ., data = df, method = "class")
plot(fit1);text(fit1, pretty = 0);
predict(fit1,type='class',test_data)
test_data$t <- predict(fit1,type='class',test_data)
View(test_data$t)
confusionMatrix(table(test$t, test_data$Attrition_Flag))
confusionMatrix(table(test_data$t, test_data$Attrition_Flag))
?confusionMatrix
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2, pretty = 0);
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),cp=.0002, method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),cp=.0002, method = "prob")
plot(fit2);text(fit2);
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='prob',test_data)
?rpart
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),control = rpart.control(cp=.0002))
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='prob',test_data)
test_data$t2 <- predict(fit2,type='class',test_data)
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
list.rules.rpart(fit1)
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.7,.3)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.3,.7)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.99,.01)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.8,.2)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.8,.1)),control = rpart.control(cp=.0002), method = "class")
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.9,.1)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.7,.3)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.87,.13)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.13,.87)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.87,.13)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
dataset <- read.csv("C:/Users/nksay/Documents/SHRM/Formatted_IndiaPulse Survey_27Sept_ProfJJ_Data.csv")
df <- dataset
df$ï..S.No <- NULL
library(ggplot2)
library(mltools)
library(car)
library(corrplot)
library(gridExtra)
str(df)
summary(df)
colSums(is.na(df))
m1 <- glm(Attrition_Flag ~ ., data = df, family = binomial(), control=glm.control(maxit=50))
str(df)
data_load <- read.csv("C:\\Users\\nksay\\Documents\\EXL Analytics\\Data.csv")
library(ROSE)
library(ggplot2)
library(corrplot)
library(MASS)
library(dplyr)
library(ROCR)
library(randomForest)
library(mice)
library(caret)
library(caTools)
library(outliers)
library(tidyverse)
library(xgboost)
library(caret)
library(Matrix)
library(data.table)
library(glmnet)
library(rpart)
data_balanced_over1 <- ovun.sample(Attrition_Flag ~ ., data = data_load, method = "under",N = 12000, na.action = na.pass)$data
str(data_balanced_over1)
## Splitting the data into training and testing data
set.seed(31)
ptr <- sample.split(data_balanced_over1$Attrition_Flag, SplitRatio = 0.7)
head(ptr)
df <- subset(data_balanced_over1, ptr == TRUE)
test_data <- subset(data_balanced_over1, ptr == FALSE)
test_data$ï..UNIQUE_ID <- NULL
df$ï..UNIQUE_ID <- NULL
## CART based DT
fit1 <- rpart(Attrition_Flag ~ ., data = df, method = "class")
plot(fit1);text(fit1, pretty = 0);
test_data$t <- predict(fit1,type='class',test_data)
confusionMatrix(table(test_data$t, test_data$Attrition_Flag))
fit2<-  rpart(Attrition_Flag ~ .,data= df,parms=list(prior=c(.87,.13)),control = rpart.control(cp=.0002), method = "class")
plot(fit2);text(fit2);
test_data$t2 <- predict(fit2,type='class',test_data)
confusionMatrix(table(test_data$t2, test_data$Attrition_Flag))
biggest <- formula(lm(EE ~. -EE1-EE2-EE3-EE4-EE5-EE6-EE7-EE8-EE9, data = df1))
no.mod <- lm(EE ~ 0, data = df1)
#Let's start with looking at the variables and framing the data as data.fram
# Making correction to data types
df$Attrition_Flag <- as.factor(df$Attrition_Flag)
df$Vol_16_D <- as.integer(df$Vol_16_D)
df$Vol_16_GeoPod_HR <- as.integer(df$Vol_16_GeoPod_HR)
df$Vol_16_F <- as.integer(df$Vol_16_F)
df$Vol_16_G <- as.integer(df$Vol_16_G)
df$Ind_2013_pushback <- as.factor(df$Ind_2013_pushback)
df$Ind_2014_pushback <- as.factor(df$Ind_2014_pushback)
df$Ind_2015_pushback <- as.factor(df$Ind_2015_pushback)
df$Ind_2016_pushback <- as.factor(df$Ind_2016_pushback)
df$Ind_2017_pushback <- as.factor(df$Ind_2017_pushback)
str(df)
##looking for NAs
sapply(df, function(x) sum(is.na(x)))
## same no. of NAs are found in Datacorp market share, Other competitors market share and Major competitiors
## There is a high chance of correlation between such variables
## Checking for correlation by dropping NAs
cor_mshare <- cor(df[,c(34:36)], use = "pairwise.complete.obs")
corrplot.mixed(cor_mshare)
## High cor between data corp and others, thus we only keep datacorp variable
df$Major_competitor_market_share <- NULL
df$Other_competitor_market_share <- NULL
init = mice(df, maxit=0)
## We will get a better understanding of the data by getting range of the continuous predictors
## removing negative revenues and volumes
df[, 1:24][df[, 1:24] < 0] <- NA
df <- na.omit(df)
df_num <- df[,c(1:28,34:35,46:58)]
# df_num <- lapply(df_num,as.numeric)
# df_num <- as.data.frame(df_num)
df_cat <- df[,c(29:33,36:45)]
cor(df_num, use = "pairwise.complete")
a <- cor(df_num, use = "pairwise.complete")
write.csv(a, "C:\\Users\\nksay\\Documents\\EXL Analytics\\correlation.csv")
## TO find association between continuous variables and the outcome we will use boxplot
ggplot(data = df_num, aes(y = df_num[,1],  col = Attrition_Flag)) + geom_boxplot() ##
## Now let us add back the outcome variable to both
df_num <- cbind(df_num,df$Attrition_Flag)
colnames(df_num)[44] <- "Attrition_Flag"
df_num$Attrition_Flag <- as.factor(df_num$Attrition_Flag)
df_cat <- cbind(df_cat,df$Attrition_Flag)
colnames(df_cat)[16] <- "Attrition_Flag"
df_cat$Attrition_Flag <- as.factor(df_cat$Attrition_Flag)
## TO find association between continuous variables and the outcome we will use boxplot
ggplot(data = df_num, aes(y = df_num[,1],  col = Attrition_Flag)) + geom_boxplot() ##
library("ggplot2")
library(viridis)
set.seed(18031)
rfmm <- read.csv("C:\\Users\\nksay\\Desktop\\Reward Received to Analytics.csv")
head(rfmm)
rfmm$Effort <- as.numeric(as.character((rfmm$Effort)))
rfmm$Support <- as.numeric(as.character((rfmm$Support)))
rfmm$Feedback <- as.numeric(as.character((rfmm$Feedback)))
rfmm$X1.on.1 <- as.numeric(as.character((rfmm$X1.on.1)))
rfmm <-na.omit(rfmm)
aa <- rfmm
str(aa)
head(rfmm)
#rfmm$ï..Leader <- NULL
summary(rfmm)
rfmm$Effort <- scale(rfmm$Effort)
rfmm$Support <- scale(rfmm$Support)
rfmm$Feedback <- scale(rfmm$Feedback)
rfmm$X1.on.1 <- scale(rfmm$X1.on.1)
str(rfmm)
nocl <- sum((nrow(rfmm[,c(-1,-3,-4,-5)])-1)*var(rfmm[,c(-1,-3,-4,-5)]))
for (i in 2:10) {
nocl[i] <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = i, iter.max = 1000)$tot.withinss
}
nocl
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 3 or 4 cluster. Let's take 3
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 6, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, shape = as.factor(a1$`ak$cluster`), col = as.factor(a1$`ak$cluster`))) + geom_point()+ scale_shape_discrete(name="Cluster", label = c("1","2","3","4","5","6")) + xlab("No. of nominations") + scale_color_discrete(name = "Cluster", label = c("1","2","3","4","5","6"))
## Finding the optimum number of clusters
nocl <- sum((nrow(rfmm[,c(-1,-3,-4,-5)])-1)*var(rfmm[,c(-1,-3,-4,-5)]))
for (i in 2:10) {
nocl[i] <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = i, iter.max = 1000)$tot.withinss
}
nocl
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 4 or 6 cluster. Let's take 4 first and then 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 4, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, shape = as.factor(a1$`ak$cluster`), col = as.factor(a1$`ak$cluster`))) + geom_point()+ scale_shape_discrete(name="Cluster", label = c("1","2","3","4","5","6")) + xlab("No. of nominations") + scale_color_discrete(name = "Cluster", label = c("1","2","3","4","5","6"))
# We can take 4 or 6 cluster. Let's take 4 first and then 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 3, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, shape = as.factor(a1$`ak$cluster`), col = as.factor(a1$`ak$cluster`))) + geom_point()+ scale_shape_discrete(name="Cluster", label = c("1","2","3","4","5","6")) + xlab("No. of nominations") + scale_color_discrete(name = "Cluster", label = c("1","2","3","4","5","6"))
# We can take 4 or 6 cluster. Let's take 4 first and then 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 8, iter.max = 1000, nstart = 25)
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 4 or 6 cluster. Let's take 4 first and then 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 8, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, shape = as.factor(a1$`ak$cluster`), col = as.factor(a1$`ak$cluster`))) + geom_point()+ scale_shape_discrete(name="Cluster", label = c("1","2","3","4","5","6")) + xlab("No. of nominations") + scale_color_discrete(name = "Cluster", label = c("1","2","3","4","5","6"))
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster", label = c("1","2","3","4","5","6"))
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
nocl
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 4 or 6 cluster. Let's take 4 first and then 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 5, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
clus1 <- subset(a1, a1$"ak$cluster"==1) ## This is read as subset of a1 where a1$column name is equal to 1
summary(clus1)## This is cluster 1 of people who have have never been nominated and still have a mid level score of manager recognition
nrow(clus1)
# We can take 4 to 6 cluster. Let's take 4 first and then try with5 and 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 6, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
#Reading the data
rfmm <- read.csv("C:\\Users\\nksay\\Desktop\\Reward Received to Analytics.csv")
head(rfmm)
rfmm$Effort <- as.numeric(as.character((rfmm$Effort)))
rfmm$Support <- as.numeric(as.character((rfmm$Support)))
rfmm$Feedback <- as.numeric(as.character((rfmm$Feedback)))
rfmm$X1.on.1 <- as.numeric(as.character((rfmm$X1.on.1)))
rfmm <-na.omit(rfmm)
aa <- rfmm
str(aa)
## Scaling data
rfmm$Effort <- scale(rfmm$Effort)
rfmm$Support <- scale(rfmm$Support)
rfmm$Feedback <- scale(rfmm$Feedback)
rfmm$X1.on.1 <- scale(rfmm$X1.on.1)
## Finding the optimum number of clusters
nocl <- sum((nrow(rfmm[,c(-1,-3,-4,-5)])-1)*var(rfmm[,c(-1,-3,-4,-5)]))
for (i in 2:10) {
nocl[i] <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = i, iter.max = 1000)$tot.withinss
}
nocl
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 4 to 6 cluster. Let's take 4 first and then try with5 and 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 6, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
# Writing this to a csv
write.csv(a1, file = "kmeans6")
nrow(clus2)
clus2 <- subset(a1, a1$`ak$cluster` == 2)
summary(clus2) # Similarly we can name other clusters based on observations
nrow(clus2)
#Reading the data
rfmm <- read.csv("C:\\Users\\nksay\\Documents\\Recognition\\Data\\Reward Received to Analytics.csv")
head(rfmm)
rfmm$Effort <- as.numeric(as.character((rfmm$Effort)))
library("ggplot2")
library(viridis)
set.seed(18031)
#Reading the data
rfmm <- read.csv("C:\\Users\\nksay\\Documents\\Recognition\\Data\\Reward Received to Analytics.csv")
head(rfmm)
rfmm$Effort <- as.numeric(as.character((rfmm$Effort)))
rfmm$Support <- as.numeric(as.character((rfmm$Support)))
rfmm$Feedback <- as.numeric(as.character((rfmm$Feedback)))
rfmm$X1.on.1 <- as.numeric(as.character((rfmm$X1.on.1)))
rfmm <-na.omit(rfmm)
aa <- rfmm
str(aa)
head(rfmm)
#rfmm$?..Leader <- NULL
summary(rfmm)
## Scaling data
rfmm$Effort <- scale(rfmm$Effort)
rfmm$Support <- scale(rfmm$Support)
rfmm$Feedback <- scale(rfmm$Feedback)
rfmm$X1.on.1 <- scale(rfmm$X1.on.1)
## Finding the optimum number of clusters
nocl <- sum((nrow(rfmm[,c(-1,-3,-4,-5)])-1)*var(rfmm[,c(-1,-3,-4,-5)]))
for (i in 2:10) {
nocl[i] <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = i, iter.max = 1000)$tot.withinss
}
nocl
plot(1:10, nocl, type="b", xlab = "no. of clusters", ylab = "tot within")
# We can take 4 to 6 cluster. Let's take 4 first and then try with5 and 6
ak <- kmeans(rfmm[,c(-1,-3,-4,-5)], centers = 6, iter.max = 1000, nstart = 25)
a1 <- cbind(aa, ak$cluster)
head(a1)
# Boxplot showing the distribution of the cluster
ggplot(aa, aes(y= Effort, x = as.factor(No..of.Times.Nominated), fill = as.factor(a1$`ak$cluster`))) + geom_boxplot() + xlab("No. of nominations") + scale_fill_discrete(name = "Cluster")
# Another visual representation of the clusters this time more clearly
ggplot(aa, aes(y= Effort, x = No..of.Times.Nominated, col = as.factor(a1$`ak$cluster`))) + geom_point()+ xlab("No. of nominations") + scale_color_discrete(name = "Cluster")
clus1 <- subset(a1, a1$"ak$cluster"==1) ## This is read as subset of a1 where a1$column name is equal to 1
summary(clus1)## This is cluster 1 of people who have have never been nominated and still have a mid level score of manager recognition
nrow(clus1)
clus2 <- subset(a1, a1$`ak$cluster` == 2)
summary(clus2) # Similarly we can name other clusters based on observations
nrow(clus2)
clus3 <- subset(a1, a1$`ak$cluster` == 3)
summary(clus3)
nrow(clus3)
clus4 <- subset(a1, a1$`ak$cluster` == 4)
summary(clus4)
nrow(clus4)
# Writing this to a csv
write.csv(a1, "C:\\Users\\nksay\\Documents\\Recognition\\Output\\kmeans6.csv")
getwd()
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
tinytex::install_tinytex()
